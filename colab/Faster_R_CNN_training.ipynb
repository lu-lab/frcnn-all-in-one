{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Faster R-CNN training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1eZM9imgixUaIfLXcpK0y0lxoS3XIB7Zh",
      "authorship_tag": "ABX9TyOdg0g+EgX7m7ZXfyD9YYzE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lu-lab/frcnn-all-in-one/blob/main/colab/Faster_R_CNN_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47FdKExYaZY1"
      },
      "source": [
        "\n",
        "# Faster R-CNN training\n",
        "---\n",
        "This notebook will allow you to train a custom object detector using Google's GPU resources. Enable this by going to Runtime -> Change runtime type and select \"GPU\" from the dropdown menu. This will speed your training time up substantially, but note that Google has a limit on how much of this GPU resource you can use. If you use the GPU resource heavily, you may have to subscribe to a paid plan. In this notebook, we will be fine-tuning the Faster-RCNN network (specifically, the Faster R-CNN Inception ResNet V2) starting from a model pre-trained on the COCO 2017 image set. This pre-trained model is provided in the Tensorflow 2 Model Zoo [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md). \n",
        "\n",
        "This notebook is meant to accompany our Jupyter notebook for annotation here [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/lu-lab/frcnn-all-in-one/HEAD). Once you have annotated data, we recommend reading this notebook through **in full** before starting. Then, you can run the cells in this notebook **in order**, being certain not to skip any unless they are marked as 'Optional'. \n",
        "\n",
        "\n",
        "###Step 00: Copy this notebook\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "First off, **save a copy of this notebook to your own Google Drive!** This is partially to protect your data and partially so you can save any changes you may want to make to this notebook. We recommend putting it into it's own folder, because we will be making quite a few sub-folders to organize the inputs and outputs of the network and the code itself.\n",
        "\n",
        "**Note**: Following this notebook will require several GB of space in your Google Drive, in addition to whatever space you may need for your annotated image data, or any data you may want to perform inferences on. \n",
        "\n",
        "\n",
        "###Step 0: Annotate your data\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Before you can train your model, you need a **set of annotations** for your images. If you do not have these already, you can use our binder here [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/lu-lab/frcnn-all-in-one/HEAD) to do that! **Once you have finished working through the binder and have downloaded the bounding_boxes.csv and label_map.pbtxt files** to your computer, return here. The images that you annotated also need to be in a folder called 'images' in the same Google Drive directory as this notebook. If you used the binder to convert a movie to images for labelling, **download the images folder as well and upload it to the folder this code is in.** Make sure the folder that contains your images is called 'images', otherwise you will run into problems later!\n",
        "\n",
        "###Step 1: Connect to your Google Drive\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Now that you're back, the first code cell below will mount Google Drive, get files we need from the GitHub repository to run this notebook, and make a few new folders that we will put data in. \n",
        "\n",
        "Before running the cell below, make sure to modify the path following the first '%cd' to the path this notebook is in! Any filepath within Google Drive starts with '/content/drive/'. To check where this notebook exists, go to File -> Locate in Drive. A new browser tab will open. Navigate to the new tab and below the 'Search' box you will see the rest of the path. For example, if you see 'My Drive > Colab Notebooks', the full path would be\n",
        "```\n",
        "/content/drive/My Drive/Colab Notebooks/\n",
        "```\n",
        "If you have spaces in the path, add a \\ before the space. For example, the path \n",
        "\n",
        "```\n",
        "/content/drive/My Drive/Colab Notebooks/Faster R-CNN/\n",
        "```\n",
        "\n",
        "becomes \n",
        "```\n",
        "/content/drive/My\\ Drive/Colab\\ Notebooks/Faster\\ R-CNN/\n",
        "```\n",
        "\n",
        "Now, make sure your Runtime type is set to GPU (Runtime -> Change runtime type and select \"GPU\" from the dropdown menu), and run the following cell. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5UD9VAxApKO"
      },
      "source": [
        "# First we need to mount Google drive and gather some dependencies...\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "\n",
        "# NOTE: Modify this path if needed\n",
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/Faster\\ R-CNN/\n",
        "working_dir = os.getcwd()\n",
        "\n",
        "!git clone https://github.com/lu-lab/frcnn-all-in-one.git\n",
        "%cd {working_dir}\n",
        "!cp -a ./frcnn-all-in-one/colab/. .\n",
        "!rm -r ./frcnn-all-in-one\n",
        "!rm Faster_R_CNN_training.ipynb\n",
        "\n",
        "# if any of these folders already exist, they will not be made, but that is not a problem\n",
        "!mkdir annotations\n",
        "!mkdir images\n",
        "!mkdir exported-model\n",
        "!mkdir inferencing-results\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9afs9QthBlsX"
      },
      "source": [
        "###Step 2: Install libraries\n",
        "\n",
        "---\n",
        "\n",
        "The next cell will install all the necessary Python libraries and packages to train our model to this notebook. At the end of this step, you'll also have several new folders, including Tensorflow, COCO-trained-model, and cocoapi. The Tensorflow directory includes most of the model development pipeline, the cocoapi allows the pipeline to compute metrics describing how well the model performs as we train it, and the COCO-trained-model is our starting model that we will be fine-tuning.\n",
        "\n",
        "If you see errors in the output to this step, note that it doesn't necessarily mean the rest of the notebook won't work. For example you might see the following errors, which you can ignore:\n",
        "```\n",
        "ERROR: multiprocess 0.70.11.1 has requirement dill>=0.3.3, but you'll have dill 0.3.1.1 which is incompatible.\n",
        "ERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.25.1 which is incompatible.\n",
        "ERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\n",
        "ERROR: apache-beam 2.30.0 has requirement avro-python3!=1.9.2,<1.10.0,>=1.8.1, but you'll have avro-python3 1.10.2 which is incompatible.\n",
        "```\n",
        "\n",
        "**Note**: If you've already run this notebook once, you do NOT need to re-download the Tensorflow, cocoapi, and COCO-trained-model files, but you DO need to install them and other libraries again if you have stopped the session! If you just need to re-install everything, put a # in front of every line in the code below that has a # at the end of it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t87xb4CyaYNr"
      },
      "source": [
        "# You will need to re-install everything if you've restarted your session\n",
        "%mkdir Tensorflow \n",
        "%cd ./Tensorflow\n",
        "!git clone https://github.com/tensorflow/models.git #\n",
        "%cd {working_dir}\n",
        "!apt-get -qq install -y protobuf-compiler python-pil python-lxml python-tk \n",
        "%cd ./Tensorflow/models/research/\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "%cd {working_dir}\n",
        "\n",
        "!git clone https://github.com/cocodataset/cocoapi.git #\n",
        "%cd ./cocoapi/PythonAPI\n",
        "!make\n",
        "coco_tools_path = os.path.join(working_dir, \"Tensorflow/models/research\")\n",
        "!cp -r pycocotools {\"%r\"%coco_tools_path}\n",
        "%cd {working_dir}\n",
        "\n",
        "%cd Tensorflow/models/research/\n",
        "!cp object_detection/packages/tf2/setup.py .\n",
        "!python -m pip install .\n",
        "%cd {working_dir}\n",
        "\n",
        "%mkdir COCO-trained-model\n",
        "%cd COCO-trained-model\n",
        "!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_inception_resnet_v2_1024x1024_coco17_tpu-8.tar.gz #\n",
        "!tar -xf faster_rcnn_inception_resnet_v2_1024x1024_coco17_tpu-8.tar.gz #\n",
        "!rm faster_rcnn_inception_resnet_v2_1024x1024_coco17_tpu-8.tar.gz #\n",
        "coco_model_path = os.path.join(working_dir, \"COCO-trained-model\")\n",
        "!mv -v faster_rcnn_inception_resnet_v2_1024x1024_coco17_tpu-8/* {\"%r\"%coco_model_path} #\n",
        "!rmdir faster_rcnn_inception_resnet_v2_1024x1024_coco17_tpu-8 #\n",
        "%cd {working_dir}\n",
        "\n",
        "!cp ./Tensorflow/models/research/object_detection/model_main_tf2.py . #\n",
        "!cp ./Tensorflow/models/research/object_detection/exporter_main_v2.py . #\n",
        "\n",
        "!pip install opencv-python-headless==4.4.0.44\n",
        "\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wx3G_i2aFgsh"
      },
      "source": [
        "### Step 3: Generate tfrecord files\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Now we want to use the bounding_boxes.csv and label_map.pbtxt file to generate tfrecords. Tfrecords are the format that tensorflow uses to input training data and to test the network on test data as the model trains. Put the 'bounding_boxes.csv' file in the 'annotations' folder and the 'label_map.pbtxt' file in the 'training' folder before running the following cell.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slsul2V_cvUB"
      },
      "source": [
        "import sys\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "sys.path.append(\"./Tensorflow/models/research/object_detection\")\n",
        "from object_detection.utils import label_map_util\n",
        "import generate_tfrecord as gt\n",
        "\n",
        "def write_tf_record(annotations, tfrecord_path, img_path, label_map):\n",
        "    tf_writer = tf.io.TFRecordWriter(tfrecord_path)\n",
        "    for annotation in annotations:\n",
        "        tf_example = gt.create_tf_example(annotation, img_path, label_map)\n",
        "        tf_writer.write(tf_example.SerializeToString())\n",
        "\n",
        "    tf_writer.close()\n",
        "    output_path = os.path.join(os.getcwd(), tfrecord_path)\n",
        "    print('Successfully created the TFRecords: {}'.format(output_path))\n",
        "\n",
        "label_map_path = './training/label_map.pbtxt'\n",
        "csv_filepath = './annotations/bounding_boxes.csv'\n",
        "\n",
        "# and now we'll convert annotations to a tfrecord\n",
        "label_map = label_map_util.get_label_map_dict(label_map_path)\n",
        "all_annotations = pd.read_csv(csv_filepath)\n",
        "img_path = './images'\n",
        "\n",
        "# tfrecord for train annotations\n",
        "tfrecord_train_path = './annotations/train.record'\n",
        "train_annotations = all_annotations[all_annotations['test_or_train'].isin(['train'])]\n",
        "grouped_train_annotations = gt.split(train_annotations, 'filename')\n",
        "write_tf_record(grouped_train_annotations, tfrecord_train_path, img_path, label_map)\n",
        "\n",
        "# tfrecord for test annotations\n",
        "tfrecord_test_path = './annotations/test.record'\n",
        "test_annotations = all_annotations[all_annotations['test_or_train'].isin(['test'])]\n",
        "grouped_test_annotations = gt.split(test_annotations, 'filename')\n",
        "write_tf_record(grouped_test_annotations, tfrecord_test_path, img_path, label_map)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RRTCy5xeS62"
      },
      "source": [
        "### Step 4: Train the model\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Now we will fine-tune the model with our own images and classes. Note that if anything is not in it's proper folder when you run this step, it will fail. You will see a lot of warnings as this step starts to run - not to worry, as long as you don't see an error it should be ok! \n",
        "\n",
        "This step will take a long time, even with the GPU. It is up to you when to stop training (you can go to Runtime ->Interrupt Execution), and generally a good rule of thumb is to watch for when the total loss starts to plateau (this may take an hour or so). We recommend starting with about an hour of training, assessing the model, then if needed you can train the model more. You should be able to monitor the loss by watching the Tensorboard widget that will start when you run the following cell. Once the first 100 training steps are complete, click the refresh icon in the upper right corner of the Tensorboard widget and you will see the board populate. You will also see a lot of outputs above the Tensorboard widget that will report the total loss. The beginning of this step is slow (on the order of ten minutes), so be patient if you're not immediately seeing outputs!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTwq00LL2DPt"
      },
      "source": [
        "import tensorflow as tf\n",
        "import datetime\n",
        "\n",
        "%tensorboard --logdir training/train/\n",
        "!python model_main_tf2.py --model_dir=training --pipeline_config_path=training/faster_rcnn.config --alsologtostderr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgvHvLdh4je3"
      },
      "source": [
        "### Step 5: Export model\n",
        "\n",
        "---\n",
        "\n",
        "Once the loss becomes reasonable and you've stopped training, freeze the model and save it to the 'exported-model' folder. You'll see quite a few warnings when you run the following cell, but again no need for concern unless you see an error. Once this is done, double check your 'exported-model' folder. It should now contain a 'checkpoint' and 'saved-model' folder and a 'pipeline.config' file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wtxkBrb5JZn"
      },
      "source": [
        "!python exporter_main_v2.py \\\n",
        "--input_type image_tensor \\\n",
        "--pipeline_config_path ./training/faster_rcnn.config \\\n",
        "--trained_checkpoint_dir ./training/ \\\n",
        "--output_directory ./exported-model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk3ioxUqObXR"
      },
      "source": [
        "### **Step 6: Test inferencing (Optional)**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Now our model is exported, we can use it to inference, or predict the bounding boxes for the classes we trained on with new images that we didn't annotate. Below are a few different options for how to do this, depending on whether the images you want inference for is a list of images, a folder of images, or a movie. This step will help you visualize detections and give you a qualitative idea of how well your model performs. You can run any combination of the cells below, and you can run each cell multiple times if you want. But, take care to rename the output files each time you run the same cell multiple times, or you may overwrite data from an earlier run! \n",
        "\n",
        "Here are the filetypes you should expect from each of the following three cells (\"Inferencing for x\"). These filetypes will be saved to the 'inferencing-results' folder:\n",
        "\n",
        "\n",
        "*   h5 (hdf) file with bounding box inferences for each image or frame in the list, folder, or movie. You can convert this to a .csv using the code cell that follows the three inferencing examples. \n",
        "*   images (.jpg format) or a movie (.mp4 format). At the end of each inferencing cell, the function 'inferencing_tools.label_all_detections_from_h5' takes as input the path of the h5 file and the paths to the original image data and uses it to overlay boxes where the model has  detected objects on the original images. If the image input is a list of image paths or a folder, the output will be in image format, and if the image input is a movie, the output will be in movie format. If you do not wish to do this for every set of images or movie you process, you can comment out the line containing 'inferencing_tools.label_all_detections_from_h5' in each of the following three cells. \n",
        "\n",
        "There are two other variables below that you may be interested in changing:\n",
        "\n",
        "\n",
        "*   ```\n",
        "target_classes\n",
        "```\n",
        "*    ```\n",
        "target_min_scores\n",
        "```\n",
        "\n",
        "```target_classes``` is a list of the classes that you would like the detector to pay attention to. This is based off of the label_map.pbtxt file that contains the names of the objects you annotated and the number that they're identified with during training. If you open the label_map.pbtxt file (which you can do from the sidebar of this notebook once your google drive is connected - click the file icon -> drive -> MyDrive -> wherever this file is ->training ->label_map.pbtxt), you will see that each class has a corresponding id. The ```target_classes``` list is index shifted, so whatever has an id of '1' in your label_map.pbtxt file must be represented in your ```target_classes``` by the number 0. If you followed our annotation notebook exactly, '0' should represent a worm and '1' should represent an egg because in the label_map.pbtxt file the worm has an id of '1' and the egg has an id of '2'. \n",
        "\n",
        " ```target_min_scores``` is used to screen detections. Whenever the model identifies an object, it also provides a score describing the confidence of the identification. A value closer to 1 is high confidence, while a value closer to 0 is low confidence. For each class, you can provide a score threshold so that you can ignore low confidence predictions. Here, for worms (or whatever corresponds to the first class in the ```target_classes``` list), we use a threshold of 0.8, and for eggs (or whatever corresponds to the second class in the ```target_classes``` list) we use a threshold of 0.3.\n",
        "\n",
        "Note that the length of the ```target_classes``` and ```target_min_scores``` lists must be equal.\n",
        "\n",
        "For convenience, we also provide a converter to save h5 data to .csv files that can be read by Excel or Google Sheets. We recommend doing this so you have an easily human-readable copy of your data. To convert your data, you can run the cell 'Convert h5 file to csv' that follows the three different inferencing cells.\n",
        "\n",
        "\n",
        "#### **Inferencing for our test images**\n",
        "First, let's see how the model performs on our annotated test images. Note that this will not inference on every image you annotated, just the ones we set aside as 'test' images. We set these images aside because it wouldn't be 'fair' if we tested out the model on the exact same images we trained it on - it wouldn't give us a realistic idea of how well the model will perform on frames from a new set of images. \n",
        "\n",
        "You do not need to alter anything in the code below to check the detections in your test images. The following code will gather all the detections for each test image and superimpose boxes indicating where the objects are that correspond to the classes in 'target_classes'. The images with boxes imposed will be displayed in the output of the following cell, and they'll be saved in the 'inferencing-results' folder, with each image having the same name as the original test image. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCpCxB9YPF7b"
      },
      "source": [
        "import sys\n",
        "import glob\n",
        "import IPython\n",
        "from IPython.display import Image, display\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from inference_code import inferencing_tools\n",
        "\n",
        "# Set up all the tensorflow files we'll need as inputs\n",
        "path_to_config = './exported-model/pipeline.config'\n",
        "path_to_ckpt = './exported-model/checkpoint'\n",
        "# we've already defined this above, but just in case\n",
        "path_to_labels = './training/label_map.pbtxt'\n",
        "\n",
        "# Add hdf file info\n",
        "save_to_hdf = True\n",
        "save_path = './inferencing-results'\n",
        "# NOTE: change the h5_file variable  so you use a different filename each time \n",
        "# you run this cell ! For example, change 'test_detections.h5' in the code below \n",
        "#to 'test_detections_2.h5' This will make sure you don't overwrite any data\n",
        "h5_file = os.path.join(save_path, 'test_detections.h5')\n",
        "\n",
        "# pull out the training image names from the bounding_boxes.csv file\n",
        "csv_filepath = './annotations/bounding_boxes.csv'\n",
        "all_annotations = pd.read_csv(csv_filepath)\n",
        "test_image_names = all_annotations['filename'][all_annotations['test_or_train'].isin(['test'])].unique()\n",
        "\n",
        "# every inference you make with this 'cnn' object will be saved in the same h5 file\n",
        "cnn = inferencing_tools.CNN_tf2(path_to_ckpt, path_to_labels, save_to_hdf, h5_file, path_to_config)\n",
        "\n",
        "# See note above on what these variables mean and how you can change them\n",
        "target_classes = [0,1]\n",
        "# Confidence score to use as a minimum threshold. Ranges from 0 to 1.\n",
        "target_min_scores = [0.8, 0.3]\n",
        "\n",
        "for fn in test_image_names:\n",
        "    image_np = cv2.imread(os.path.join('./images', fn))\n",
        "    #inferencing happens in this call - the h5 file will store information using the 'path' variable as part of the key\n",
        "    boxes = cnn.get_detections(image_np, fn, target_classes, target_min_scores)\n",
        "    print('Processing image %s' % fn)\n",
        "\n",
        "test_image_paths = [os.path.join('./images', im_name) for im_name in test_image_names]\n",
        "# Now that we have all the detections, label them on the test data and visualize the detections on each test image.\n",
        "inferencing_tools.label_all_detections_from_h5(h5_file, test_image_paths, save_path, target_classes)\n",
        "\n",
        "# The following two lines will display your test images with boxes superimposed\n",
        "# in this notebook. \n",
        "for image_name in glob.glob('./inferencing-results/*.jpg'): \n",
        "    display(Image(filename=image_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd_JveUNVUTC"
      },
      "source": [
        "#### **Inferencing from a folder of images**\n",
        "This is very similar to the example above. The difference is that here we detect all objects in classes 1 and 2 from .png or .jpg images in a folder ```image_dir```. If you followed our annotation notebook exactly, these classes would be eggs and worms.  Edit the ```image_dir``` variable below to lead to the folder with your data! The data must be on Google Drive.\n",
        "\n",
        "Here we again save the boxes and scores to an h5 file in the 'inferencing-results' directory. We then overlay the detections on top of the original images and save them in the 'inferencing-results' folder with the same name as the original image. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-QX1UzEVf-U"
      },
      "source": [
        "import sys\n",
        "import glob\n",
        "import IPython\n",
        "from IPython.display import Image, display\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from inference_code import inferencing_tools\n",
        "\n",
        "# set up inputs to the inferencer\n",
        "path_to_config = './exported-model/pipeline.config'\n",
        "path_to_ckpt = './exported-model/checkpoint'\n",
        "path_to_labels = './training/label_map.pbtxt'\n",
        "save_to_hdf = True\n",
        "save_path = './inferencing-results'\n",
        "# Note! each time you run this cell, you should change the 'folder_detections.h5' part of the filename\n",
        "# otherwise you may overwrite data! The name can be anything you want, e.g. 'folder_detections_2.h5'\n",
        "h5_file = os.path.join(save_path, 'folder_detections.h5')\n",
        "\n",
        "# every inference you make with this 'cnn' object will be saved in the same h5 file\n",
        "cnn = inferencing_tools.CNN_tf2(path_to_ckpt, path_to_labels, save_to_hdf, h5_file, path_to_config)\n",
        "\n",
        "# take a look at qualitatively how well the model performs on images in the 'image_dir' folder\n",
        "# edit the 'image_dir' variable below to reflect the where your folder with data is!\n",
        "image_dir = './images'\n",
        "ext_list = ['.jpg', '.png'] # if you have another file format, you can add it here. most formats should work.\n",
        "image_names = [f for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))\n",
        "                             and f.endswith(tuple(ext_list))]\n",
        "\n",
        "# See note above on what these variables mean and how you can change them\n",
        "target_classes = [0,1]\n",
        "# Confidence score to use as a minimum threshold. Ranges from 0 to 1.\n",
        "target_min_scores = [0.8, 0.3]\n",
        "\n",
        "for fn in image_names:\n",
        "    image_np = cv2.imread(os.path.join(image_dir, fn))\n",
        "    #inferencing happens in this call - you can directly use the boxes this returns if you wish. They are also saved in the h5 file\n",
        "    boxes = cnn.get_detections(image_np, fn, target_classes, target_min_scores)\n",
        "    print('Processing image %s' % fn)\n",
        "\n",
        "# Now that we have all the detections, label them on the test data and visualize the detections on each test image.\n",
        "image_paths = [os.path.join(image_dir, fn) for fn in image_names]\n",
        "inferencing_tools.label_all_detections_from_h5(h5_file, image_paths, save_path, target_classes)\n",
        "\n",
        "# superimpose boxes on images\n",
        "for image_name in glob.glob('./inferencing-results/*.jpg'): \n",
        "    display(Image(filename=image_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awzXEbVNVukB"
      },
      "source": [
        "#### **Inferencing from a movie**\n",
        "Here's an example that detects both eggs and worms for each frame in a video with the path assigned to ```movie_path``` and saves the detections to an h5 file.  The movie with detections overlaid can then be created with the filename assigned to the ```save_file``` variable. The ```movie_path``` must lead to a movie uploaded to Google Drive.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjisdN9YV4Ez"
      },
      "source": [
        "import sys\n",
        "import glob\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from inference_code import inferencing_tools\n",
        "\n",
        "# detect and visualize detections from movie\n",
        "# change this variable to wherever your movie is stored\n",
        "movie_path = 'your-video-path.mp4'\n",
        "save_file = './inferencing-results/test_nn.mp4'\n",
        "\n",
        "path_to_config = './exported-model/pipeline.config'\n",
        "path_to_ckpt = './exported-model/checkpoint'\n",
        "# we've already defined this above, but just in case\n",
        "path_to_labels = './training/label_map.pbtxt'\n",
        "save_to_hdf = True\n",
        "save_path = './inferencing-results'\n",
        "# Note! each time you run this cell, you should change the 'movie_detections.h5' part of the filename\n",
        "# otherwise you may overwrite data! The name can be anything you want, e.g. 'movie_detections_2.h5'\n",
        "h5_file = os.path.join(save_path, 'movie_detections.h5')\n",
        "\n",
        "# every inference you make with this 'cnn' object will be saved in the same h5 file\n",
        "cnn = inferencing_tools.CNN_tf2(path_to_ckpt, path_to_labels, save_to_hdf, h5_file, path_to_config)\n",
        "\n",
        "# See the note above on these variables.\n",
        "target_classes = [0, 1]\n",
        "# Confidence score to use as a minimum threshold. Ranges from 0 to 1.\n",
        "target_min_scores = [0.8, 0.3]\n",
        "\n",
        "vid = cv2.VideoCapture(movie_path)\n",
        "idx = 1\n",
        "while vid.isOpened():\n",
        "    ret, image = vid.read()\n",
        "    if ret:\n",
        "        #inferencing happens in this call\n",
        "        boxes = cnn.get_detections(image, idx, target_classes, target_min_scores)\n",
        "    else:\n",
        "        break\n",
        "    print(\"Processing frame no %s\" % idx)\n",
        "    idx += 1\n",
        "vid.release()\n",
        "\n",
        "# if the input of the inferencing is a video, the output will be a video\n",
        "inferencing_tools.label_all_detections_from_h5(h5_file, movie_path, save_file, target_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjqQZFD8Vot8"
      },
      "source": [
        "#### **Convert h5 file to csv**\n",
        "\n",
        "Before running this cell, you **must** run one of the inferencing cells above, or you'll get an error.\n",
        "\n",
        "By replacing the path assigned to ```h5_file```\n",
        "below with the path to your own h5 file, you can convert it to a csv file. The current set up would allow you to convert the file ```'./inferencing-results/folder_detections.h5'``` to a csv file ```'./inferencing-results/bounding_boxes_from_folder.csv'```. If you used the inferencing cell above that performs inferencing on a folder of images with the default naming, the cell below will work as is, otherwise, simply modify the ```h5_file``` variable. The default location of the ```csv_filepath``` will save to the 'inferencing-results' folder, but feel free to rename or relocate this file as you like. \n",
        "\n",
        "\n",
        "In the resulting file, the 'frame' column is either the movie frame number or image name, and the 'xmin', 'xmax', 'ymin', and 'ymax' columns are all expressed as a proportion of the total size of the image or frame (e.g. the upper left corner of a box should be ($x_{min}*width_{image}$, $y_{min}* height_{image}$) and the lower right corner should be ($x_{max}*width_{image}$, $y_{max}* height_{image}$)). In the class column is an integer that corresponds to the classes in your label_map.pbtxt file. If you used the same classes in the annotation notebook, then class 1 is a worm and class 2 is an egg. Finally, the 'score' column is an indication of the confidence that the model has in this detection, with scores closer to 1 being highly confident and scores close to zero having very low confidence. \n",
        "\n",
        "If you wish to detect classes other than class 1 and 2 from the label_map.pbtxt file, modify the list of ```target_classes``` below to reflect the index-shifted classes (as described at the top of Step 6) you would like to be saved to the csv file. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K4wM9ySWJXt"
      },
      "source": [
        "import h5py\n",
        "import pandas as pd\n",
        "from inference_code import inferencing_tools\n",
        "from object_detection.utils import label_map_util\n",
        "\n",
        "save_path = './inferencing-results'\n",
        "# Note: change this name so it corresponds to the h5 file you want to convert\n",
        "h5_file = os.path.join(save_path, 'folder_detections.h5')\n",
        "target_classes = [0, 1]\n",
        "\n",
        "xmins = []\n",
        "xmaxs = []\n",
        "ymins = []\n",
        "ymaxs = []\n",
        "classes = []\n",
        "scores = []\n",
        "frame_no = []\n",
        "\n",
        "\n",
        "with h5py.File(h5_file, 'r') as hf:\n",
        "  keys = list(hf.keys())\n",
        "  # look for the identifier at the end of the key - for movies this is the frame number, \n",
        "  # for images it is the image name\n",
        "  frames = [txt.split('frame_')[-1] for txt in keys]\n",
        "  for frame in frames:\n",
        "    # get info about frame\n",
        "    xmin, ymin, xmax, ymax, id, score = inferencing_tools.get_boxes_from_h5(frame, hf, target_classes)\n",
        "    xmins.extend(xmin)\n",
        "    ymins.extend(ymin)\n",
        "    xmaxs.extend(xmax)\n",
        "    ymaxs.extend(ymax)\n",
        "    classes.extend(id)\n",
        "    scores.extend(score)\n",
        "    frame_no.extend([frame]*len(score))\n",
        "\n",
        "data = {'frame': frame_no, 'xmin': xmins, 'xmax':xmaxs, 'ymin':ymins, \n",
        "        'ymax':ymaxs, 'class':classes, 'score':scores}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# save csv with detections to 'inferencing-results' directory\n",
        "csv_filepath = './inferencing-results/bounding_boxes_from_folder.csv'\n",
        "df.to_csv(csv_filepath, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dnqzdwh1l4qx"
      },
      "source": [
        "### Step 7: Training more\n",
        "\n",
        "---\n",
        "\n",
        "If you're not satisifed with your model after training, it's straightforward to continue from where you left off. You can also add more data to your training set at this point. Here's how you would go about that. \n",
        "\n",
        "\n",
        "\n",
        "1.   If you want to add more training data, annotate more images using our annotation notebook here [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/lu-lab/frcnn-all-in-one/HEAD). \n",
        " \n",
        "  *   Make sure your classes are the same (the order they are listed in the Binder matters!).\n",
        "  *   Just like before, download 'bounding_boxes.csv' and any images you may have converted from movies. The names of the images should not overlap with any images that are already in the 'images' directory in this Google Drive folder.\n",
        "  *   Upload the new images into the 'images' directory in this Google Drive folder\n",
        "  *   Add the new bounding_boxes data to your 'bounding_boxes.csv' file that is in the 'annotations' folder in this directory. You can open both files in Google Sheets, and copy the new annotation data (from row 2 on down) into the old 'bounding_boxes.csv' that is in this notebook. Make sure to save the file as a csv! When you opeen the file in Google Sheets, edits will automatically be saved in a Sheets file, not in the csv file. \n",
        "  *   Delete the train.record and test.record files in the 'annotations' folder. \n",
        "\n",
        " \n",
        "2.   Continue by working Steps 1-3 in this notebook again. \n",
        "\n",
        "3.   Before re-starting training, we need to edit the 'faster_rcnn.config' file in the 'training' folder. Go to line 104 of the file, which currently says \n",
        "```fine_tune_checkpoint: \"COCO-trained-model/checkpoint/ckpt-0\"```\n",
        "\n",
        "  Assuming you want to start from the model you previously exported, you would change this line to \n",
        "```fine_tune_checkpoint: \"exported-model/checkpoint/ckpt-0\"```\n",
        "\n",
        "4.   Delete all the files in the 'training' directory **except** the faster_rcnn.config file and the label_map.pbtxt file.\n",
        "\n",
        "5.   Continue by working the remaining steps in this notebook.  "
      ]
    }
  ]
}