{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Faster_R_CNN_inferencing.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lu-lab/frcnn-all-in-one/blob/main/colab/Faster_R_CNN_inferencing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47FdKExYaZY1"
      },
      "source": [
        "\n",
        "# Faster R-CNN inferencing\n",
        "---\n",
        "This notebook will allow you to inference using a pre-trained object detector and Google's GPU resources. Enable GPU by going to Runtime -> Change runtime type and select \"GPU\" from the dropdown menu. This will speed your inferencing time up substantially, but note that Google has a limit on how much of this GPU resource you can use. If you use the GPU resource heavily, you may have to subscribe to a paid plan. \n",
        "\n",
        "We recommend reading this notebook through **in full** before starting. Then, you can run the cells in this notebook. The cells in Step 0-Step 2 must be run **in order**. The actual inferencing, performed in Step 3, includes multiple cells, of which you can run any or all before moving to Step 4. \n",
        "\n",
        "**Note**: If the session disconnects, you will have to re-run all the cells leading up to inferencing.\n",
        "\n",
        "###Step 0: Copy this notebook and download pre-trained models\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "First off, **save a copy of this notebook to your own Google Drive!** This is partially to protect your data and partially so you can save any changes you may want to make to this notebook. We recommend putting it into it's own folder, because we will be making a few sub-folders to organize the inputs and outputs of the network and the code.\n",
        "\n",
        "At this point you should also **download the pre-trained model you'd like to test**. Once you've downloaded the model, extract it if needed. Then make a folder called 'model' in the same folder as this notebook in your Google Drive. For models created with Tensorflow 1 , you'll need to upload the label_map.pbtxt file and the frozen_inference_graph.pb file. For models created with Tensorflow 2 (for example, any models trained in our other notebook), you'll need to upload the label_map.pbtxt and 'saved_model' folder. The models we fine-tuned for our paper can be found [here](https://doi.org/10.6084/m9.figshare.13678705.v1), and were all created with Tensorflow 1. Here's a quick guide on what models from the paper we'd recommend trying for what imaging conditions:\n",
        "\n",
        "*   The **developmental model** (referenced as worms-on-plate in the paper) tends to work best for imaging conditions where the worm is relatively small and on an agar plate, seeded or unseeded. \n",
        "*   The **egg model** (referenced as egg-finder in the paper) tends to work best for data where the worm and/or eggs are imaged at relatively high resolution.\n",
        "*   The **aging model** is highly tuned for animals in a specific microfluidic device and in our experience it does not generalize as well as the other models.However, if you believe your data look similar to the data we use with this model in our paper (see Figures 1 and 4), it may be worth a try. \n",
        "\n",
        "\n",
        "You can also find many pre-trained models [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md) and [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md) at the Tensorflow 1 and 2 model zoos. \n",
        "\n",
        "**Note**: Following this notebook will require several GB of space in your Google Drive, in addition to whatever space you may need for any image data you may want to perform inferencing on. \n",
        "\n",
        "\n",
        "###Step 1: Connect to your Google Drive\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The first code cell below will mount Google Drive, get files we need from the GitHub repository to run this notebook, and make a new folder that we will put inferencing results in. \n",
        "\n",
        "Before running the cell below, make sure to modify the path following the first '%cd' to the path this notebook is in! Any filepath within Google Drive starts with '/content/drive/'. To check where this notebook exists, go to File -> Locate in Drive. A new browser tab will open. Navigate to the new tab and below the 'Search' box you will see the rest of the path. For example, if you see 'My Drive > Colab Notebooks', the full path would be\n",
        "```\n",
        "/content/drive/My Drive/Colab Notebooks/\n",
        "```\n",
        "If you have spaces in the path, add a \\ before the space. For example, the path \n",
        "\n",
        "```\n",
        "/content/drive/My Drive/Colab Notebooks/Faster R-CNN/\n",
        "```\n",
        "\n",
        "becomes \n",
        "```\n",
        "/content/drive/My\\ Drive/Colab\\ Notebooks/Faster\\ R-CNN/\n",
        "```\n",
        "Now, make sure your Runtime type is set to GPU (Runtime -> Change runtime type and select \"GPU\" from the dropdown menu), and run the following cell. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5UD9VAxApKO"
      },
      "source": [
        "# First we need to mount Google drive and gather some dependencies...\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "\n",
        "# NOTE: Modify this path if needed\n",
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/Faster\\ R-CNN\\ inferencing/\n",
        "working_dir = os.getcwd()\n",
        "\n",
        "!git clone https://github.com/lu-lab/frcnn-all-in-one.git\n",
        "%cd {working_dir}\n",
        "!cp -a ./frcnn-all-in-one/colab/. .\n",
        "!rm -r ./frcnn-all-in-one\n",
        "# Since you already have the inferencing notebook in your google drive, delete the second copy that is in the repository\n",
        "!rm Faster_R_CNN_inferencing.ipynb\n",
        "\n",
        "# if this folder already exists, it will not be made, but that should be ok.\n",
        "!mkdir inferencing-results\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9afs9QthBlsX"
      },
      "source": [
        "###Step 2: Install libraries\n",
        "\n",
        "---\n",
        "\n",
        "The next cell will install all the necessary Python libraries and packages to inference using a pre-trained model in this notebook. \n",
        "\n",
        "If you see errors in the output to this step, note that it doesn't necessarily mean the rest of the notebook won't work. For example you might see the following errors, which you can ignore:\n",
        "```\n",
        "ERROR: multiprocess 0.70.11.1 has requirement dill>=0.3.3, but you'll have dill 0.3.1.1 which is incompatible.\n",
        "ERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.25.1 which is incompatible.\n",
        "ERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\n",
        "ERROR: apache-beam 2.30.0 has requirement avro-python3!=1.9.2,<1.10.0,>=1.8.1, but you'll have avro-python3 1.10.2 which is incompatible.\n",
        "```\n",
        "\n",
        "If you've already run this notebook before, you will already have the Tensorflow models repository in a folder called Tensorflow and you can prevent re-cloning that repo by using a # to comment out the line \n",
        "```!git clone https://github.com/tensorflow/models.git #```. However, every time you restart the notebook, you do need to re-install each of the libraries below, so don't eliminate anything else. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t87xb4CyaYNr"
      },
      "source": [
        "# You will need to re-install everything if you've restarted your session\n",
        "!pip install --upgrade pip\n",
        "\n",
        "%mkdir Tensorflow \n",
        "%cd ./Tensorflow\n",
        "!git clone https://github.com/tensorflow/models.git #\n",
        "%cd {working_dir}\n",
        "!apt-get -qq install -y protobuf-compiler python-pil python-lxml python-tk \n",
        "%cd ./Tensorflow/models/research/\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "%cd {working_dir}\n",
        "\n",
        "%cd Tensorflow/models/research/\n",
        "!cp object_detection/packages/tf2/setup.py .\n",
        "!python -m pip install .\n",
        "%cd {working_dir}\n",
        "\n",
        "!pip install opencv-python-headless==4.4.0.44"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk3ioxUqObXR"
      },
      "source": [
        "### **Step 3: Test inferencing**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Now we've set up the environment, we can use the pre-trained model to inference, or predict the bounding boxes for the classes the model is trained on with images we didn't train on. Below are a few different options for how to do this, depending on whether the images you want inferences for is a list of images, a folder of images, or a movie. This step will help you visualize detections and give you a qualitative idea of how well the model performs with your data. **You can run one or multiple of the cells below to inference with different types of input data.** But, take care to rename the output files each time you run the same cell multiple times, or you may overwrite data from an earlier run! \n",
        "\n",
        "Here are the filetypes you should expect from each of the following three cells (\"Inferencing for x\"). These filetypes will be saved to the 'inferencing-results' folder:\n",
        "\n",
        "\n",
        "*   h5 (hdf) file with bounding box inferences for each image or frame in the list, folder, or movie. You can convert this to a .csv using the code cell that follows the three inferencing examples. \n",
        "*   images (.jpg format) or a movie (.mp4 format). At the end of each inferencing cell, the function 'inferencing_tools.label_all_detections_from_h5' takes as input the path of the h5 file and the paths to the original image data and uses it to overlay boxes where the model has  detected objects on the original images. If the image input is a list of image paths or a folder, the output will be in image format, and if the image input is a movie, the output will be in movie format. If you do not wish to do this for every set of images or movie you process, you can comment out the line containing 'inferencing_tools.label_all_detections_from_h5' in each of the following three cells. \n",
        "\n",
        "There are two other variables below that you may be interested in changing:\n",
        "\n",
        "\n",
        "*   ```\n",
        "target_classes\n",
        "```\n",
        "*    ```\n",
        "target_min_scores\n",
        "```\n",
        "\n",
        "```target_classes``` is a list of the classes that you would like the detector to pay attention to. This is based off of the label_map.pbtxt file that contains the names of the objects you annotated and the number that they're identified with during training. If you open the label_map.pbtxt file (which you can do from the sidebar of this notebook once your google drive is connected - click the file icon -> drive -> MyDrive -> wherever this file is ->training ->label_map.pbtxt), you will see that each class has a corresponding id. The ```target_classes``` list is index shifted, so whatever has an id of '1' in your label_map.pbtxt file must be represented in your ```target_classes``` by the number 0. If you followed our annotation notebook exactly, '0' should represent a worm and '1' should represent an egg because in the label_map.pbtxt file the worm has an id of '1' and the egg has an id of '2'. \n",
        "\n",
        " ```target_min_scores``` is used to screen detections. Whenever the model identifies an object, it also provides a score describing the confidence of the identification. A value closer to 1 is high confidence, while a value closer to 0 is low confidence. For each class, you can provide a score threshold so that you can ignore low confidence predictions. Here, for worms (or whatever corresponds to the first class in the ```target_classes``` list), we use a threshold of 0.8, and for eggs (or whatever corresponds to the second class in the ```target_classes``` list) we use a threshold of 0.3.\n",
        "\n",
        "Note that the length of the ```target_classes``` and ```target_min_scores``` lists must be equal.\n",
        "\n",
        "For convenience, we also provide a converter to save h5 data to .csv files that can be read by Excel or Google Sheets. We recommend doing this so you have an easily human-readable copy of your data. To convert your data, you can run the cell 'Convert h5 file to csv' that follows the three different inferencing cells.\n",
        "\n",
        "In each example below, we first get inferences for each image or frame in the list, folder, or movie and store it in an h5 file. This filetype is convenient because the whole file need not be loaded into memory in order to access variables. Then, we use the boxes and scores in the h5 file to draw boxes around where the model detects worms or eggs (or whatever classes are assigned to labels 1 and 2). For convenience, we also provide a converter to save h5 data to .csv files that can be read by Excel or Google Sheets. \n",
        "\n",
        "The inferencing for models created with Tensorflow 1 and Tensorflow 2 is a little different. All the examples below show how to inference using a Tensorflow 1 model, but you can follow the instructions below to modify a couple lines so that you can inference using a Tensorflow 2 model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1KOKy2fGfL4"
      },
      "source": [
        "#### **Adapting each cell for Tensorflow 2 models**\n",
        "All the examples below are designed for use with Tensorflow 1 models, since the models from our paper are Tensorflow 1 models. However, you can easily adapt this to Tensorflow 2 models by changing a few lines in each of the cells below. First, find the lines \n",
        "\n",
        "```\n",
        "cnn = inferencing_tools.CNN_tf1(path_to_frozen_graph, path_to_labels, save_to_hdf, h5_file) #comment for Tensorflow 2 models\n",
        "# path_to_config = './model/pipeline.config' #uncomment for Tensorflow 2 models\n",
        "# path_to_ckpt = './model/checkpoint' #uncomment for Tensorflow 2 models\n",
        "# cnn = inferencing_tools.CNN_tf2(path_to_frozen_graph, path_to_labels, save_to_hdf, h5_file) #uncomment for Tensorflow 2 models\n",
        "```\n",
        "To switch to Tensorflow 2, use a # to comment out the first of these two lines, and remove the # from the following three lines. \n",
        "\n",
        "This assumes that your model is saved in the folder 'model'\n",
        "\n",
        "#### **Inferencing for a list of images**\n",
        "First, let's see how the model performs by feeding in a list of image names. The images themselves are in the 'data' -> 'test_images' folder. If you've run the cells above, you'll see a file 'image_list.xlsx'. This file contains a list with the paths to the images you want to inference in a column, with 'filename' labelling the column in the first row. Once we gather all the detections for each image in the list, we'll superimpose boxes indicating where the objects are, with red boxes being worms and blue boxes being eggs (assuming worms and eggs correspond to classes 1 and 2). The images with boxes imposed will be displayed below the following cell, and they'll be saved in the 'inferencing-results' folder, with each image having the same name as the original image. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCpCxB9YPF7b"
      },
      "source": [
        "import sys\n",
        "import glob\n",
        "import IPython\n",
        "from IPython.display import Image, display\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from inference_code import inferencing_tools\n",
        "\n",
        "# get Tensorflow model files that we need\n",
        "path_to_frozen_graph = './model/saved_model/'\n",
        "# we've already defined this above, but just in case\n",
        "path_to_labels = './model/label_map.pbtxt'\n",
        "\n",
        "# where to save?\n",
        "save_to_hdf = True\n",
        "save_path = './inferencing-results'\n",
        "# NOTE: change the h5_file variable  so you use a different filename each time \n",
        "# you run this cell ! For example, change 'test_detections.h5' in the code below \n",
        "#to 'test_detections_2.h5' This will make sure you don't overwrite any data\n",
        "h5_file = os.path.join(save_path, 'test_detections.h5')\n",
        "\n",
        "# pull out the training image names\n",
        "excel_filepath = './data/image_list.xlsx'\n",
        "images = pd.read_excel(excel_filepath)\n",
        "test_image_paths = images['filename']\n",
        "\n",
        "# every inference you make with this 'cnn' object will be saved in the same h5 file\n",
        "cnn = inferencing_tools.CNN_tf1(path_to_frozen_graph, path_to_labels, save_to_hdf, h5_file) #comment for Tensorflow 2 models\n",
        "\n",
        "# path_to_config = './model/pipeline.config' #uncomment for Tensorflow 2 models\n",
        "# path_to_ckpt = './model/checkpoint' #uncomment for Tensorflow 2 models\n",
        "# cnn = inferencing_tools.CNN_tf2(path_to_ckpt, path_to_labels, save_to_hdf, h5_file, path_to_config) #uncomment for Tensorflow 2 models\n",
        "\n",
        "# See note above on what these variables mean and how you can change them\n",
        "target_classes = [0,1]\n",
        "# Confidence score to use as a minimum threshold. Ranges from 0 to 1.\n",
        "target_min_scores = [0.8, 0.3]\n",
        "\n",
        "for fn in test_image_paths:\n",
        "    image_np = cv2.imread(fn)\n",
        "    key = os.path.basename(fn)\n",
        "    #inferencing happens in this call - the h5 file will store information using the 'path' variable as part of the key\n",
        "    boxes = cnn.get_detections(image_np, key, target_classes, target_min_scores)\n",
        "    print('Processing image %s' % fn)\n",
        "\n",
        "# Now that we have all the detections, label them on the test data and visualize the detections on each test image.\n",
        "inferencing_tools.label_all_detections_from_h5(h5_file, test_image_paths.to_list(), save_path, target_classes)\n",
        "\n",
        "# The following two lines will display your test images with boxes superimposed\n",
        "# in this notebook. \n",
        "for image_name in glob.glob('./inferencing-results/*.jpg'): \n",
        "    display(Image(filename=image_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd_JveUNVUTC"
      },
      "source": [
        "#### **Inferencing from a folder of images**\n",
        "This is very similar to the example above. The difference is that here we detect all objects in classes 1 and 2 from .png or .jpg images in a folder ```image_dir```. Edit the ```image_dir``` variable below to lead to the folder with your data! The data must be on Google Drive.\n",
        "\n",
        "Here we again save the boxes and scores to an h5 file in the 'inferencing-results' directory. We then overlay the detections on top of the original images and save them in the 'inferencing-results' folder with the same name as the original image. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-QX1UzEVf-U"
      },
      "source": [
        "import sys\n",
        "import glob\n",
        "import IPython\n",
        "from IPython.display import Image, display\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from inference_code import inferencing_tools\n",
        "\n",
        "# get Tensorflow model files we need\n",
        "path_to_frozen_graph = './model/saved_model'\n",
        "path_to_labels = './model/label_map.pbtxt'\n",
        "\n",
        "# where to save?\n",
        "save_to_hdf = True\n",
        "save_path = './inferencing-results'\n",
        "# Note! each time you run this cell, you should change the 'folder_detections.h5' part of the filename\n",
        "# otherwise you may overwrite data! The name can be anything you want, e.g. 'folder_detections_2.h5'\n",
        "h5_file = os.path.join(save_path, 'folder_detections.h5')\n",
        "\n",
        "# every inference you make with this 'cnn' object will be saved in the same h5 file\n",
        "cnn = inferencing_tools.CNN_tf1(path_to_frozen_graph, path_to_labels, save_to_hdf, h5_file) #comment for Tensorflow 2 models\n",
        "\n",
        "# path_to_config = './model/pipeline.config' #uncomment for Tensorflow 2 models\n",
        "# path_to_ckpt = './model/checkpoint' #uncomment for Tensorflow 2 models\n",
        "# cnn = inferencing_tools.CNN_tf2(path_to_ckpt, path_to_labels, save_to_hdf, h5_file, path_to_config) #uncomment for Tensorflow 2 models\n",
        "\n",
        "# take a look at qualitatively how well the model performs on images in the 'image_dir' folder\n",
        "# edit the 'image_dir' variable below to reflect the where your data is stored!\n",
        "image_dir = './data/test_images'\n",
        "ext_list = ['.jpg', '.png'] # if you have another file format, you can add it here. most formats should work.\n",
        "image_names = [f for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))\n",
        "                             and f.endswith(tuple(ext_list))]\n",
        "\n",
        "# See note above on what these variables mean and how you can change them\n",
        "target_classes = [0,1]\n",
        "# Confidence score to use as a minimum threshold. Ranges from 0 to 1.\n",
        "target_min_scores = [0.8, 0.3]\n",
        "\n",
        "for fn in image_names:\n",
        "    image_np = cv2.imread(os.path.join(image_dir, fn))\n",
        "    #inferencing happens in this call - you can directly use the boxes this returns if you wish. They are also saved in the h5 file\n",
        "    boxes = cnn.get_detections(image_np, fn, target_classes, target_min_scores)\n",
        "    print('Processing image %s' % fn)\n",
        "\n",
        "# Now that we have all the detections, label them on the test data and visualize the detections on each test image.\n",
        "image_paths = [os.path.join(image_dir, fn) for fn in image_names]\n",
        "inferencing_tools.label_all_detections_from_h5(h5_file, image_paths, save_path, target_classes)\n",
        "\n",
        "# show images in the notebook\n",
        "for image_name in glob.glob('./inferencing-results/*.jpg'): \n",
        "    display(Image(filename=image_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awzXEbVNVukB"
      },
      "source": [
        "#### **Inferencing from a movie**\n",
        "Here's an example that detects both eggs and worms ( or whatever objects correspond to classes 1 and 2) for each frame in a video with the path assigned to ```movie_path``` and saves the detections to an h5 file.  The movie with detections overlaid can then be created with the filename assigned to the ```save_file``` variable. The ```movie_path``` must lead to a movie uploaded to Google Drive.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjisdN9YV4Ez"
      },
      "source": [
        "import sys\n",
        "import glob\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from inference_code import inferencing_tools\n",
        "\n",
        "# detect and visualize detections from movie\n",
        "save_to_hdf = True\n",
        "save_path = './inferencing-results'\n",
        "# Note! each time you run this cell, you should change the 'movie_detections.h5' part of the filename\n",
        "# otherwise you may overwrite data! The name can be anything you want, e.g. 'movie_detections_2.h5'\n",
        "h5_file = os.path.join(save_path, 'movie_detections.h5')\n",
        "\n",
        "# the input can be most video formats, the output file should be an mp4 file if\n",
        "# you wish to view the video in this notebook.\n",
        "movie_path = 'your-video-path.mp4'\n",
        "save_file = './inferencing-results/test_nn.mp4'\n",
        "\n",
        "# Tensorflow model files that we need\n",
        "path_to_frozen_graph = './model/saved_model'\n",
        "# we've already defined this above, but just in case\n",
        "path_to_labels = './model/label_map.pbtxt'\n",
        "\n",
        "\n",
        "# every inference you make with this 'cnn' object will be saved in the same h5 file\n",
        "cnn = inferencing_tools.CNN_tf1(path_to_frozen_graph, path_to_labels, save_to_hdf, h5_file) #comment for Tensorflow 2 models\n",
        "\n",
        "# path_to_config = './model/pipeline.config' #uncomment for Tensorflow 2 models\n",
        "# path_to_ckpt = './model/checkpoint' #uncomment for Tensorflow 2 models\n",
        "#cnn = inferencing_tools.CNN_tf2(path_to_frozen_graph, path_to_labels, save_to_hdf, h5_file) #uncomment for Tensorflow 2 models\n",
        "\n",
        "# See the note above on these variables.\n",
        "target_classes = [0, 1]\n",
        "# Confidence score to use as a minimum threshold. Ranges from 0 to 1.\n",
        "target_min_scores = [0.8, 0.3]\n",
        "\n",
        "vid = cv2.VideoCapture(movie_path)\n",
        "idx = 1\n",
        "while vid.isOpened():\n",
        "    ret, image = vid.read()\n",
        "    if ret:\n",
        "        #inferencing happens in this call\n",
        "        boxes = cnn.get_detections(image, idx, target_classes, target_min_scores)\n",
        "    else:\n",
        "        break\n",
        "    print(\"Processing frame no %s\" % idx)\n",
        "    idx += 1\n",
        "vid.release()\n",
        "\n",
        "# if the input of the inferencing is a video, the output will be a video\n",
        "inferencing_tools.label_all_detections_from_h5(h5_file, movie_path, save_file, target_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjqQZFD8Vot8"
      },
      "source": [
        "### **Step 4: Convert h5 file to csv**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Before running this cell, you **must** run one of the inferencing cells above, or you'll get an error. \n",
        "\n",
        "By replacing the path assigned to ```h5_file```\n",
        "below with the path to your own h5 file, you can convert it to a csv file. The current set up would allow you to convert the file ```'./inferencing-results/folder_detections.h5'``` to a csv file ```'./inferencing-results/bounding_boxes_from_folder.csv'```. If you used the inferencing cell above that performs inferencing on a folder of images with the default naming, the cell below will work as is, otherwise, simply modify the ```h5_file``` variable. The default location of the ```csv_filepath``` will save to the 'inferencing-results' folder, but feel free to rename or relocate this file as you like. \n",
        "\n",
        "\n",
        "In the resulting file, the 'frame' column is either the movie frame or image name, and the 'xmin', 'xmax', 'ymin', and 'ymax' columns are all expressed as a proportion of the total size of the image or frame (e.g. the upper left corner of a box should be (xmin *x* image_width, ymin *x* image_width) ). In the class column is an integer that corresponds to the classes in your label_map.txt file. If you used the same classes in the annotation notebook, then class 1 is a worm and class 2 is an egg. Finally, the 'score' column is an indication of the confidence that the model has in this detection, with scores closer to 1 being highly confident and scores close to zero having very low confidence. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K4wM9ySWJXt"
      },
      "source": [
        "import h5py\n",
        "import pandas as pd\n",
        "from inference_code import inferencing_tools\n",
        "\n",
        "save_path = './inferencing-results'\n",
        "# Note: change this name so it corresponds to the h5 file you want to convert\n",
        "h5_file = os.path.join(save_path, 'folder_detections.h5')\n",
        "target_classes = [0, 1]\n",
        "\n",
        "xmins = []\n",
        "xmaxs = []\n",
        "ymins = []\n",
        "ymaxs = []\n",
        "classes = []\n",
        "scores = []\n",
        "frame_no = []\n",
        "\n",
        "\n",
        "with h5py.File(h5_file, 'r') as hf:\n",
        "  keys = list(hf.keys())\n",
        "  # look for the identifier at the end of the key - for movies this is the frame number, \n",
        "  # for images it is the image name\n",
        "  frames = [txt.split('frame_')[-1] for txt in keys]\n",
        "  for frame in frames:\n",
        "    # get info about frame\n",
        "    xmin, ymin, xmax, ymax, id, score = inferencing_tools.get_boxes_from_h5(frame, hf, target_classes)\n",
        "    xmins.extend(xmin)\n",
        "    ymins.extend(ymin)\n",
        "    xmaxs.extend(xmax)\n",
        "    ymaxs.extend(ymax)\n",
        "    classes.extend(id)\n",
        "    scores.extend(score)\n",
        "    frame_no.extend([frame]*len(score))\n",
        "\n",
        "data = {'frame': frame_no, 'xmin': xmins, 'xmax':xmaxs, 'ymin':ymins, \n",
        "        'ymax':ymaxs, 'class':classes, 'score':scores}\n",
        "df = pd.DataFrame(data).drop_duplicates()\n",
        "\n",
        "# save csv with detections to 'inferencing-results' directory\n",
        "csv_filepath = './inferencing-results/bounding_boxes_from_folder.csv'\n",
        "df.to_csv(csv_filepath, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dnqzdwh1l4qx"
      },
      "source": [
        "### Step 5: Training (Optional)\n",
        "\n",
        "---\n",
        "\n",
        "If you're not satisifed with the pre-trained model's performance, it's straightforward to train a model by following the directions in our binder notebook [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/lu-lab/frcnn-all-in-one/HEAD) and colab training notebook [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lu-lab/frcnn-all-in-one/blob/main/colab/Faster_R_CNN_training.ipynb). \n",
        "\n",
        "\n"
      ]
    }
  ]
}